{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트레인 이미지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "# 가위 이미지 불러오기\n",
    "image_path = \"rock_scissors_paper/scissors\"\n",
    "resize_images(image_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "바위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "# 바위 이미지 불러오기\n",
    "\n",
    "image_path = \"rock_scissors_paper/rock\"\n",
    "resize_images(image_path)\n",
    "\n",
    "print(\"바위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "# 보 이미지 불러오기\n",
    "\n",
    "image_path = \"rock_scissors_paper/paper\"\n",
    "resize_images(image_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 300 입니다.\n",
      "x_train shape: (300, 28, 28, 3)\n",
      "y_train shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  \n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    img_color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*img_color,dtype=np.int32).reshape(number_of_data,img_size,img_size,img_color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    \n",
    "    for file in glob.iglob(img_path+'/scissors/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "        \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_path = \"rock_scissors_paper\"\n",
    "(x_train, y_train)=load_data(image_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[27])\n",
    "print('라벨: ', y_train[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 네트워크 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수 :  7\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape =(28,28,3)))\n",
    "                             #얼마나 다양한 이미지의 특징을 살펴볼 것인가?               #color 이미지\n",
    "                             #(입력 이미지가 다양할 수록 더 많은 특징을 고려)\n",
    "                             #필터 개수 -> 현재는 한 이미지에서 16개의 특징을 뽑아냄\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "                            #분류기 알고리즘을 얼마나 복잡하게 할 것인가?\n",
    "                            #(복잡한 문제일수록 이 수를 늘림)\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "                            #최종 분류기의 class 수\n",
    "                            #여기서는 총 3개의 class를 구분하므로 3\n",
    "        \n",
    "\n",
    "print('Model에 추가된 Layer 개수 : ', len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 30,819\n",
      "Trainable params: 30,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 17ms/step - loss: 1.1187 - accuracy: 0.3332\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.9989 - accuracy: 0.5847\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.8501 - accuracy: 0.6814\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.6898 - accuracy: 0.7682\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.6058 - accuracy: 0.7830\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.5003 - accuracy: 0.8245\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.4166 - accuracy: 0.8460\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.3255 - accuracy: 0.9183\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.2978 - accuracy: 0.9075\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 34ms/step - loss: 0.2511 - accuracy: 0.9333\n",
      "10/10 - 0s - loss: 0.2831 - accuracy: 0.9200\n",
      "train_loss: 0.2830555737018585\n",
      "train_accuracy: 0.9200000166893005\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Nadam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs=10)\n",
    "\n",
    "train_loss, train_accuracy = model.evaluate(x_train_norm, y_train, verbose =2)\n",
    "\n",
    "print(\"train_loss: {}\".format(train_loss))\n",
    "print(\"train_accuracy: {}\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 이미지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  images to be resized.\n",
      "100  images resized.\n",
      "가위 이미지 resize 완료!\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "바위 이미지 resize 완료!\n",
      "100  images to be resized.\n",
      "100  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = \"test/scissors\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")\n",
    "\n",
    "# 바위 이미지 불러오기\n",
    "\n",
    "image_dir_path = \"test/rock\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"바위 이미지 resize 완료!\")\n",
    "\n",
    "# 보 이미지 불러오기\n",
    "image_dir_path = \"test/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가데이터(x_test)의 이미지 개수는 300 입니다.\n",
      "x_test shape: (300, 28, 28, 3)\n",
      "y_test shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=300):  \n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissors/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "        \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"평가데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = \"test\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 15ms/step - loss: 1.8399 - accuracy: 0.3100\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9593 - accuracy: 0.5134\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 0.8460 - accuracy: 0.5860\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.6886 - accuracy: 0.6718\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 0.6712 - accuracy: 0.6903\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.6199 - accuracy: 0.6993\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.5385 - accuracy: 0.7739\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.5184 - accuracy: 0.7917\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.4785 - accuracy: 0.7764\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.4571 - accuracy: 0.8073\n",
      "10/10 - 0s - loss: 0.4043 - accuracy: 0.8267\n",
      "test_loss: 0.40427589416503906\n",
      "test_accuracy: 0.8266666531562805\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Nadam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_test_norm, y_test, epochs=10)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test_norm, y_test, verbose =2)\n",
    "\n",
    "print(\"test_loss: {}\".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer별 트레인/테스트 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Adam : train : 0.9399 / test : 0.7933\n",
    "- AdaGrad : train : 0.4900 / test : 0.6200\n",
    "- AdaMax : train : 0.7800 / test : 0.6633\n",
    "- RMSprop : train : 0.9467  / test : 0.8000\n",
    "- Nadam : train : 0.9200 / test : 0.8266\n",
    "\n",
    "머신러닝처럼 따로 feature engineering을 할 부분이 없기 때문에, 수작업으로 성능 향상을 위해 할 수 있는 것을 찾아 보았고 가장 먼저 하이퍼 파라미터 튜닝을 시도해 보았다. 구글링을 통해 다양한 optimizer를 찾았고 하나씩 대입해본 결과 'Nadam'으로 했을 때 테스트 성능이 가장 좋았다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape\n",
    "\n",
    "흑백으로 처리를 하면 학습 속도가 더 빨라질 것으로 생각해 컬러 이미지를 흑백으로 바꿀 수 있도록 reshape 하였다.\n",
    "그렇게 될 경우 x의 shape가 (900, 28, 28, 1)로 바뀌어서 y의 shape과 맞지 않아 에러가 발생했다. 3차원을 1차원으로 바꾸는 것이기 때문에 (300, 28, 28, 3)이 (900, 28, 28, 1)로 변하는 것은 불가피했고, 300으로 유지한 채 흑백(1)으로 변환하는 방법을 찾지 못했다. 따라서 LMS 실습과는 달리 x에 대해서 reshape를 진행하지 않았다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense의 크기와 Conv2D의 크기가 왜 2의 제곱으로 커지는 지\n",
    "\n",
    "Dense의 크기와 Conv2D의 크기가 왜 2의 제곱으로 커지는 지 궁금해서 찾아보았다.\n",
    "conv2d 등의 레이어를 적용할 경우 피쳐 맵의 사이즈가 2로 나누어지고, pooling 진행할 때 4로 나누어지기 때문에 2의 제곱수로 커진다는 것을 유추해볼 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 이미지 데이터를 만들어서 CNN을 진행한 건 처음이어서 학습할 데이터를 만드는 것에서 부터 다양한 생각이 들었다. 손의 모양을 컴퓨터가 잘 학습할 수 있도록 뚜렷하게 사진을 찍어야 할 지, 가까이서 찍어야 할 지, 멀리서 찍어야 할 지 등등 단순히 데이터를 받아오고 구글링을 해서 코드를 완성하는 것보다 재미있었다. 이번 과제 한 번으로 CNN을 명확히 알게된 건 아니지만 큰 흐름은 파악할 수 있었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**향후 시도해보고 싶은 것**\n",
    "\n",
    "- 랜덤 서치로 하이퍼 파라미터 튜닝하기\n",
    "- keras의 다른 모델도 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
